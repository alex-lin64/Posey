{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import mediapipe as mp\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 33 keypoints are in order\n",
    "\n",
    "0 - nose \\\n",
    "1 - left eye (inner) \\\n",
    "2 - left eye \\\n",
    "3 - left eye (outer) \\\n",
    "4 - right eye (inner) \\\n",
    "5 - right eye \\\n",
    "6 - right eye (outer) \\\n",
    "7 - left ear \\\n",
    "8 - right ear \\\n",
    "9 - mouth (left) \\\n",
    "10 - mouth (right) \\\n",
    "11 - left shoulder \\\n",
    "12 - right shoulder \\\n",
    "13 - left elbow \\\n",
    "14 - right elbow \\\n",
    "15 - left wrist \\\n",
    "16 - right wrist \\\n",
    "17 - left pinky \\\n",
    "18 - right pinky \\\n",
    "19 - left index \\\n",
    "20 - right index \\\n",
    "21 - left thumb \\\n",
    "22 - right thumb \\\n",
    "23 - left hip \\\n",
    "24 - right hip \\\n",
    "25 - left knee \\\n",
    "26 - right knee \\\n",
    "27 - left ankle \\\n",
    "28 - right ankle \\\n",
    "29 - left heel \\\n",
    "30 - right heel \\\n",
    "31 - left foot index \\\n",
    "32 - right foot index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For squat detection, we don't need to the keypoints 1, 3, 4, and 5 (too much info about eyes), \\\n",
    "9 and 10 (don't need to know mouth position), and 17-22 (info about fingers too detailed, only \\\n",
    "need wrist keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Keypoints to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_pose(keypoints, filepath, classification):\n",
    "    \"\"\"\n",
    "    Cleans keypoints data, writes file to csv.  \n",
    "\n",
    "    :params:\n",
    "        - keypoints: landmark obj of all 33 keypoints w/ x, y, z, and visibility, \n",
    "            already normalized\n",
    "        - filepath: path of csv file to write to\n",
    "        - classification: 1 if up position, 0 if down position\n",
    "    \"\"\"\n",
    "    # clean up unwanted keypoints\n",
    "    keypoints = keypoints[:1] + keypoints[2:3] + keypoints[6:9] + keypoints[11:17] + keypoints[23:33]\n",
    "    # flatten to 1d vector\n",
    "    cleaned_kp = np.array([[x.x, x.y, x.z] for x in keypoints], dtype=\"float64\").flatten() \n",
    "    cleaned_kp = np.append(cleaned_kp, classification)\n",
    "    cleaned_kp = cleaned_kp.reshape(1, len(cleaned_kp))\n",
    "    df = pd.DataFrame(cleaned_kp)\n",
    "\n",
    "    # write data to cvs\n",
    "    file_exists = os.path.isfile(filepath)\n",
    "    if not file_exists:\n",
    "        df.to_csv(filepath, index=False)\n",
    "    else:\n",
    "        df.to_csv(filepath, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "video_path = \"data\\\\test_videos\\\\Video 8-31-23, 10 34 48.mov\"\n",
    "save_path = \"data\\\\test_csv\\\\squat_data_test_2.csv\"\n",
    "\n",
    "# starts video capture on video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "csv_filepath = save_path\n",
    "\n",
    "# runs mediapipe pose detection\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # changing display colors\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame.flags.writeable = False\n",
    "        results = pose.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        cv2.imshow(\"Webcam Feed\", frame)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('u'):\n",
    "            # captures keypoints for up position\n",
    "            capture_pose(results.pose_landmarks.landmark, csv_filepath, 1)\n",
    "        elif k == ord('d'):\n",
    "            # captures keypoints for down position\n",
    "            capture_pose(results.pose_landmarks.landmark, csv_filepath, 0)\n",
    "        elif k == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(csv_filepath):\n",
    "    \"\"\"\n",
    "    Splits a testing csv dataset into 80/20 split of testing/validation\n",
    "\n",
    "    :params:\n",
    "        - csv_filepath: path to csv file\n",
    "\n",
    "    :returns:\n",
    "        - X_train: pandas df for training, datapoints\n",
    "        - x_val: pandas df for training, class labels\n",
    "        - Y_train:pandas df for validation, datapoints\n",
    "        - y_val: pandas df for validation, class labels\n",
    "    \"\"\"\n",
    "    # create df from data\n",
    "    squat_df = pd.read_csv(csv_filepath)\n",
    "    # split data into train and val\n",
    "    squat_df['split'] = np.random.randn(squat_df.shape[0], 1)\n",
    "    mask = np.random.rand(len(squat_df)) <= 0.8\n",
    "\n",
    "    # training dataset\n",
    "    X_train = squat_df[mask].astype('float64')\n",
    "    X_val = squat_df[~mask].astype('float64')\n",
    "    # corresponding labels\n",
    "    y_train = keras.utils.to_categorical(X_train.pop('63'))\n",
    "    y_val = keras.utils.to_categorical(X_val.pop('63'))\n",
    "    # pop_off_split\n",
    "    X_train.pop('split')\n",
    "    X_val.pop('split')\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def landmarks_to_embedding(landmarks):\n",
    "    \"\"\"\n",
    "    Converts input landarks to pose embedding\n",
    "\n",
    "    :params:\n",
    "        - landmarks: pandas df of the landmarks of dim (21, 3)\n",
    "\n",
    "    :returns:\n",
    "        - The landmarks as an embedded vector of dim (51)\n",
    "    \"\"\"\n",
    "    # reshape into matrix with dims (21, 3)\n",
    "    reshaped_input = keras.layers.Reshape((21, 3))(landmarks)\n",
    "    return keras.layers.Flatten()(reshaped_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(63))\n",
    "embedding = landmarks_to_embedding(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "\n",
    "outputs = keras.layers.Dense(2, activation='softmax')(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = 'data\\\\train_csv\\\\squat_data_train_2.csv'\n",
    "\n",
    "X_train, y_train, X_val, y_val = split_train_test(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# checkpoint to store highest validation accuracy\n",
    "checkpoint_path = 'model\\\\weights\\\\ weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "# start training\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[checkpoint, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize & Analyze Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDIT TO: https://www.tensorflow.org/lite/tutorials/pose_classification\n",
    "\n",
    "# Visualize loss curves\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with test dataset\n",
    "test_data_csv_path = 'data\\\\test_csv\\\\squat_data_test_2.csv'\n",
    "\n",
    "# create df from data\n",
    "X_test = pd.read_csv(test_data_csv_path)\n",
    "y_test = keras.utils.to_categorical(X_test.pop('63'))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Model To TFLite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDIT TO https://www.tensorflow.org/lite/tutorials/pose_classification\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "print('Model size: %dKB' % (len(tflite_model) / 1024))\n",
    "\n",
    "model_name = 'squat_classifier_v2.tflite'\n",
    "model_path = os.path.join(\"model\\\\tf_lite_model\", model_name)\n",
    "with open(model_path, 'wb+') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tflite model for accuracy: CREDIT TO https://www.tensorflow.org/lite/tutorials/pose_classification\n",
    "\n",
    "def evaluate_model(interpreter, X, y_true):\n",
    "  \"\"\"Evaluates the given TFLite model and return its accuracy.\"\"\"\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on all given poses.\n",
    "  y_pred = []\n",
    "  for i in range(len(y_true)):\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = X[i: i + 1].astype('float32')\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the class with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    predicted_label = np.argmax(output()[0])\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  y_pred = keras.utils.to_categorical(y_pred)\n",
    "  return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Evaluate the accuracy of the converted TFLite model\n",
    "classifier_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "classifier_interpreter.allocate_tensors()\n",
    "print('Accuracy of TFLite model: %s' %\n",
    "      evaluate_model(classifier_interpreter, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
